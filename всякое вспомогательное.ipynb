{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c30172d-e55a-4f0d-b3ce-7346c1d69fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement mpl_toolkits (from versions: none)\n",
      "ERROR: No matching distribution found for mpl_toolkits\n"
     ]
    }
   ],
   "source": [
    "!pip install mpl_toolkits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5bcd66-4d91-4d0b-8b72-68f2080833f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ae5f6-2774-4d55-91dd-9bc5e2224479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "def analyzePCA(X, pca, explained_threshold=0.95):\n",
    "    \"\"\"Plot cumulative explaned variance of PCA\"\"\"\n",
    "    pca.fit(X)\n",
    "    pca_variance = pca.explained_variance_ratio_\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(np.cumsum(pca_variance))\n",
    "    plt.hlines(explained_threshold, 0, len(pca_variance))\n",
    "    plt.title(\"Number of components to explain {}% of variance: {}\".format(\n",
    "        100*explained_threshold, np.argwhere(np.cumsum(pca_variance)>0.95)[0]\n",
    "    ))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance');\n",
    "\n",
    "# feats_lst = []\n",
    "# for i in feats_woe:\n",
    "#     if i.find('cross_') != 0:\n",
    "#         feats_lst.append(i)\n",
    "        \n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_scaled = scaler.fit_transform(df.drop(['DDateRequest', 'demand_id1', 'RequestIDRR', 'NPL30'], axis = 1))\n",
    "\n",
    "pca = PCA()\n",
    "analyzePCA(df_scaled, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b09e63-9ee4-4fab-b290-ddff54c089a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tune_sklearn import TuneSearchCV\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3)\n",
    "\n",
    "# A parameter grid for XGBoost\n",
    "# params = {\n",
    "#     \"min_child_weight\": list(range(2, 25)),\n",
    "#     \"gamma\": list(np.arange(0.1, 5, 0.1)),\n",
    "#     \"subsample\": list(np.arange(0.1, 0.9, 0.025)),\n",
    "#     \"colsample_bytree\": list(np.arange(0.1, 0.9, 0.05)),\n",
    "#     \"max_depth\": list(range(2, 15)),\n",
    "#     \"n_estimators\": list(range(3, 250)),\n",
    "#     \"learning_rate\": [0.0001, 0.001, 0.01, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    'n_estimators'     : list(range(2, 3000)),  # noqa: E221,E241\n",
    "    'max_depth'        : list(range(2, 4)),   # noqa: E221,E241\n",
    "    'gamma'            : list(np.arange(0.1, 5, 0.1)),  # noqa: E221,E241\n",
    "    'eta'              : list(np.arange(0.001, 0.5, 0.001)),  # noqa: E221,E241\n",
    "    'colsample_bytree' : list(np.arange(0.005, 0.995, 0.005)),    # noqa: E221,E241\n",
    "    'subsample'        : list(np.arange(0.005, 0.995, 0.005)),    # noqa: E221,E241\n",
    "    'reg_lambda'       : list(np.arange(0.1, .95, 0.05)),  # noqa: E221,E241\n",
    "    'min_child_weight' : list(range(2, 125)),   # noqa: E221,E241\n",
    "    'max_leaves'       : list(range(2, 125)),  # noqa: E221,E241\n",
    "    \"learning_rate\": [0.0001, 0.001, 0.01, 0.1, 0.15, 0.16, 0.17, 0.18, 0.19]\n",
    "}\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.02,\n",
    "    n_estimators=50,\n",
    "    objective=\"binary:logistic\",\n",
    "    nthread=5,\n",
    "    # tree_method=\"gpu_hist\"  # this enables GPU.\n",
    "    # See https://github.com/dmlc/xgboost/issues/2819\n",
    ")\n",
    "\n",
    "digit_search = TuneSearchCV(\n",
    "    xgb,\n",
    "    param_distributions=params,\n",
    "    n_trials=3,\n",
    "    # use_gpu=True # Commented out for testing on github actions,\n",
    "    # but this is how you would use gpu\n",
    ")\n",
    "\n",
    "digit_search.fit( df[df['oot_flg'] == 0][cols], df[df['oot_flg'] == 0][target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365056fc-5520-4e90-9aae-11757eb9f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Дан список названий колонок column_list и датасет df, \n",
    "в котором колонки частично совпадают со списком. Напиши код, \n",
    "который создаст новый датасет df1 с колонками из списка column_list, \n",
    "перенесет в новый датасет df1 все колонки, которые есть в списке из \n",
    "исходного df, и те, которых нет, заполнит нулями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8a2119-8c37-45a7-a1a1-269813fd259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A    B  C    D  E\n",
      "0  1  NaN  4  NaN  7\n",
      "1  2  NaN  5  NaN  8\n",
      "2  3  NaN  6  NaN  9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Пример данных\n",
    "column_list = ['A', 'B', 'C', 'D', 'E']\n",
    "data = {\n",
    "    'A': [1, 2, 3],\n",
    "    'C': [4, 5, 6],\n",
    "    'E': [7, 8, 9]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Создание нового датасета df1 с колонками из column_list и заполнением нулями\n",
    "df1 = pd.DataFrame(columns=column_list)  # Создание датасета с заданными колонками\n",
    "\n",
    "for column in df.columns:\n",
    "    if column in column_list:\n",
    "        df1[column] = df[column]  # Перенос совпадающих колонок\n",
    "    else:\n",
    "        df1[column] = 0  # Заполнение отсутствующих колонок нулями\n",
    "\n",
    "# Вывод нового датасета df1\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b973b-7336-4875-8b1f-9ad661d0baf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321f891-b99b-4b28-af00-f5bb37237fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d8171-6bf3-47cf-a556-06ee4924520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря с уникальными списками товаров в одном кластере\n",
    "product_clusters = {}\n",
    "for _, row in cluster_groups.iterrows():\n",
    "    cluster_id = row['cluster']\n",
    "    product_ids = row['ID товара']\n",
    "    for product_id in product_ids:\n",
    "        if product_id not in product_clusters:\n",
    "            product_clusters[product_id] = []\n",
    "\n",
    "        # Добавление уникальных товаров в список без дубликатов\n",
    "        for pid in product_ids:\n",
    "            if pid != product_id and pid not in product_clusters[product_id]:\n",
    "                product_clusters[product_id].append(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986bdede-e783-40a0-a5ea-8ad194521e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Как работает эта часть кода:\n",
    "\n",
    "1. **product_clusters**:\n",
    "   - Создается пустой словарь product_clusters, который будет содержать соответствие ID товара и списков товаров в одном кластере.\n",
    "\n",
    "2. **Итерация по группам кластеров**:\n",
    "   - Цикл for _, row in cluster_groups.iterrows() проходит по каждой строке в группах кластеров. Здесь row представляет собой строку из DataFrame.\n",
    "\n",
    "3. **Извлечение ID кластера и товаров**:\n",
    "   - Извлекается ID кластера (cluster_id) и список ID товаров (product_ids) из текущей строки.\n",
    "\n",
    "4. **Формирование соответствий в словаре**:\n",
    "   - Для каждого товара в списке product_ids проверяется, есть ли он уже в словаре product_clusters. Если нет, создается пустой список для этого товара.\n",
    "   - Затем добавляются ID всех товаров из списка product_ids, за исключением самого товара, путем расширения списка текущим списком, используя список включений (list comprehension).\n",
    "\n",
    "Например, если в кластере с ID 1 есть товары с ID [101, 102, 103], то после выполнения этой части кода словарь product_clusters будет иметь вид:\n",
    "{\n",
    "    101: [102, 103],\n",
    "    102: [101, 103],\n",
    "    103: [101, 102]\n",
    "}\n",
    "\n",
    "\n",
    "Таким образом, этот участок кода формирует соответствие ID товара и списков товаров из одного и того же кластера в словаре product_clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f77e4-d94a-48d1-ae3b-70eba4ef83ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Чтобы избежать дубликатов в списках товаров в словаре product_clusters, \n",
    "можно модифицировать код таким образом, чтобы при добавлении товаров в \n",
    "список выполнялась проверка на их уникальность. Вот как можно отредактировать эту часть кода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559293f-9b93-4dce-b3a5-c87c75c5705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Изменения в коде:\n",
    "\n",
    "1. **Дополнительная проверка для уникальности**:\n",
    "   - Добавлена внутренняя проверка при добавлении товаров в \n",
    "список в рамках одного кластера. Перед добавлением товара в список проверяется, не содержится ли он там уже.\n",
    "\n",
    "Теперь код будет добавлять только уникальные товары в список для каждого товара в словаре.\n",
    "Это поможет избежать дубликатов и создать структуру, в которой каждый товар из одного кластера не повторяется в своем же списке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5a900-d13a-4738-be69-79717515aff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d2713-bd36-4148-92da-4b2916182de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45cbfd-ef44-4255-93b9-c8c30231b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Загрузка данных из датасета\n",
    "# Предполагается, что данные уже загружены в DataFrame df\n",
    "# Пример: df = pd.read_csv(\"file.csv\")\n",
    "\n",
    "# Группировка данных по кластерам\n",
    "cluster_groups = df.groupby('cluster')['ID товара'].apply(list).reset_index()\n",
    "\n",
    " Создание словаря с уникальными списками товаров в одном кластере\n",
    "product_clusters = {}\n",
    "for _, row in cluster_groups.iterrows():\n",
    "    cluster_id = row['cluster']\n",
    "    product_ids = row['ID товара']\n",
    "    for product_id in product_ids:\n",
    "        if product_id not in product_clusters:\n",
    "            product_clusters[product_id] = []\n",
    "\n",
    "        # Добавление уникальных товаров в список без дубликатов\n",
    "        for pid in product_ids:\n",
    "            if pid != product_id and pid not in product_clusters[product_id]:\n",
    "                product_clusters[product_id].append(pid)\n",
    "\n",
    "# Преобразование в JSON\n",
    "import json\n",
    "json_data = json.dumps(product_clusters, indent=4)\n",
    "\n",
    "# Запись JSON в файл\n",
    "with open('product_clusters.json', 'w') as json_file:\n",
    "    json_file.write(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f832-953f-4a7c-8d37-64ed2bb4dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    " Имеется датасет pandas , \n",
    "    первая колонка - ID товара, со второй по пятую - \n",
    "    значения категорий товаров с первой по четвертую степень вложенности. \n",
    "    в шестой  - номер кластера. Нужно получить файл json, в котором каждому\n",
    "    id товара будет соответствовать список из всех ID товара, которые находятся\n",
    "    с ним в одних и тех же кластерах,  чтобы внутри каждого списка товаров в словаре не было дубликатов. \n",
    "    Каждый ID товара может входить в несколько разных кластеров.\n",
    "    Дальше нужно проверить каждый ключ словаря на вхождение товара в группу товаров из списка  по соответствию категории уровня 2)\n",
    "    Каждый список нужно пропустить через фильтр на соответствие категории товаров списку соответствующему разметке, в которой ключ\n",
    "    - категория товара из ключа, значения - разные по уровню иерархии категории ( значения из одной из колонок 2-5) исходного датасета\n",
    "    Напиши скрипт на Python для реализации этой задачи и поясни все части кода\n",
    "   \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa1010-02bc-474a-b1bc-281b57814fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Создание тестового датасета\n",
    "data = {\n",
    "    'ID товара': [1, 2, 3],\n",
    "    'Кат1': ['A', 'B', 'C'],\n",
    "    'Кат2': ['M', 'N', 'O'],\n",
    "    'Кат3': ['X', 'Y', 'Z'],\n",
    "    'Кат4': ['P', 'Q', 'R'],\n",
    "    'Кластер': [1, 2, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Шаг 1: Преобразование датасета в словарь\n",
    "cluster_dict = df.groupby('Кластер')['ID товара'].apply(list).to_dict()\n",
    "\n",
    "# Шаг 2: Создание списка соответствий категорий товаров\n",
    "category_mapping = {\n",
    "    'A': ['M', 'N', 'X'],\n",
    "    'B': ['M', 'N', 'Y'],\n",
    "    'C': ['M', 'O', 'Z']\n",
    "}\n",
    "\n",
    "# Шаг 3: Фильтрация товаров по категориям\n",
    "result = {}\n",
    "for cluster, products in cluster_dict.items():\n",
    "    filtered_products = []\n",
    "    for product in products:\n",
    "        categories = df.loc[df['ID товара'] == product, 'Кат1':'Кат4'].values.flatten().tolist()\n",
    "        if all(category_mapping.get(categories[1]) == categories[2:]):\n",
    "            filtered_products.append(product)\n",
    "    result[cluster] = filtered_products\n",
    "\n",
    "# Шаг 4: Генерация JSON файла\n",
    "with open('result.json', 'w') as file:\n",
    "    json.dump(result, file, ensure_ascii=False, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b272e-00e2-41a9-9e52-dbd1ec5eae7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
